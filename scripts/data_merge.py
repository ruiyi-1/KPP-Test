#!/usr/bin/env python3
"""
KPPé¢˜ç›®æ•°æ®æ±‡æ€»è„šæœ¬
åŠŸèƒ½ï¼šå°†æ‰€æœ‰Partçš„é¢˜ç›®æ±‡æ€»ä¸ºç»Ÿä¸€é¢˜åº“ï¼Œå»é™¤Partåˆ†ç±»
"""

import json
from pathlib import Path
from typing import Dict, List, Set, Tuple
import hashlib

# é…ç½®
DATA_DIR = Path(__file__).parent.parent / "data"
QUESTIONS_DIR = DATA_DIR / "questions"
TRANSLATIONS_DIR = DATA_DIR / "translations"
OUTPUT_FILE = DATA_DIR / "questions.json"
OUTPUT_TRANSLATIONS_FILE = TRANSLATIONS_DIR / "zh.json"

def calculate_question_hash(question_data: Dict) -> str:
    """è®¡ç®—é¢˜ç›®çš„å“ˆå¸Œå€¼ï¼Œç”¨äºå»é‡"""
    # ä½¿ç”¨é¢˜ç›®æ–‡æœ¬å’Œé€‰é¡¹æ–‡æœ¬è®¡ç®—å“ˆå¸Œ
    key_parts = [
        question_data.get("question_text", ""),
    ]
    for option in question_data.get("options", []):
        key_parts.append(option.get("text", ""))
    
    key_string = "|".join(key_parts)
    return hashlib.md5(key_string.encode("utf-8")).hexdigest()

def convert_to_final_format(question_data: Dict, new_id: str) -> Dict:
    """è½¬æ¢ä¸ºæœ€ç»ˆæ•°æ®åº“ç»“æ„æ ¼å¼"""
    # ç¡®å®šé¢˜ç›®ç±»å‹
    has_image_options = question_data.get("has_image_options", False)
    question_type = "image-options" if has_image_options else "text"
    
    # è½¬æ¢é€‰é¡¹æ ¼å¼
    options = []
    for option in question_data.get("options", []):
        option_type = "image" if option.get("has_image", False) else "text"
        option_dict = {
            "type": option_type,
            "label": option.get("label", ""),
            "content": option.get("text", ""),
        }
        if option.get("image"):
            option_dict["imagePath"] = option["image"]
        options.append(option_dict)
    
    # æ„å»ºæœ€ç»ˆæ ¼å¼
    final_data = {
        "id": new_id,
        "question": question_data.get("question_text", ""),
        "questionType": question_type,
        "options": options,
        "correctAnswer": question_data.get("correct_answer"),
        "questionImages": question_data.get("question_images", [])
    }
    
    return final_data

def merge_questions() -> Tuple[List[Dict], Dict]:
    """æ±‡æ€»æ‰€æœ‰Partçš„é¢˜ç›®"""
    question_files = sorted(QUESTIONS_DIR.glob("part-*-question-*.json"))
    
    if not question_files:
        print("âš ï¸  æœªæ‰¾åˆ°é¢˜ç›®æ–‡ä»¶")
        return [], {}
    
    print(f"ğŸ“‚ æ‰¾åˆ° {len(question_files)} ä¸ªé¢˜ç›®æ–‡ä»¶")
    
    all_questions = []
    seen_hashes: Set[str] = set()
    question_id_map = {}  # æ—§ID -> æ–°IDæ˜ å°„
    
    question_counter = 1
    
    # æŒ‰Partå’Œé¢˜ç›®ç¼–å·æ’åº
    def get_sort_key(file_path: Path) -> tuple:
        name = file_path.stem
        # æå– part å’Œ question_number
        parts = name.split("-")
        if len(parts) >= 3:
            part = parts[1].upper()
            question_num = int(parts[3]) if parts[3].isdigit() else 0
            return (part, question_num)
        return ("", 0)
    
    sorted_files = sorted(question_files, key=get_sort_key)
    
    for question_file in sorted_files:
        try:
            with open(question_file, "r", encoding="utf-8") as f:
                question_data = json.load(f)
            
            # è®¡ç®—å“ˆå¸Œå€¼æ£€æŸ¥é‡å¤
            question_hash = calculate_question_hash(question_data)
            if question_hash in seen_hashes:
                print(f"  âš ï¸  è·³è¿‡é‡å¤é¢˜ç›®: {question_file.name}")
                continue
            
            seen_hashes.add(question_hash)
            
            # ç”Ÿæˆæ–°IDï¼ˆå»é™¤Partå‰ç¼€ï¼‰
            new_id = f"question-{question_counter:03d}"
            old_id = question_data.get("id", "")
            question_id_map[old_id] = new_id
            
            # è½¬æ¢ä¸ºæœ€ç»ˆæ ¼å¼
            final_question = convert_to_final_format(question_data, new_id)
            all_questions.append(final_question)
            
            question_counter += 1
            
        except Exception as e:
            print(f"  âŒ å¤„ç†å¤±è´¥ {question_file.name}: {e}")
            import traceback
            traceback.print_exc()
    
    return all_questions, question_id_map

def update_translations(question_id_map: Dict[str, str]) -> Dict:
    """æ›´æ–°ç¿»è¯‘æ–‡ä»¶ä¸­çš„é¢˜ç›®IDå¼•ç”¨"""
    translation_file = TRANSLATIONS_DIR / "zh.json"
    
    if not translation_file.exists():
        print("âš ï¸  ç¿»è¯‘æ–‡ä»¶ä¸å­˜åœ¨ï¼Œè·³è¿‡æ›´æ–°")
        return {}
    
    try:
        with open(translation_file, "r", encoding="utf-8") as f:
            translations = json.load(f)
        
        updated_translations = {}
        questions = translations.get("questions", {})
        
        for old_id, new_id in question_id_map.items():
            if old_id in questions:
                updated_translations[new_id] = questions[old_id]
        
        return {"questions": updated_translations}
        
    except Exception as e:
        print(f"âš ï¸  æ›´æ–°ç¿»è¯‘æ–‡ä»¶å¤±è´¥: {e}")
        return {}

def validate_merged_data(questions: List[Dict]) -> Tuple[bool, List[str]]:
    """éªŒè¯æ±‡æ€»åçš„æ•°æ®"""
    errors = []
    
    if not questions:
        errors.append("æ²¡æœ‰é¢˜ç›®æ•°æ®")
        return False, errors
    
    # æ£€æŸ¥IDå”¯ä¸€æ€§
    ids = [q.get("id") for q in questions]
    if len(ids) != len(set(ids)):
        errors.append("å­˜åœ¨é‡å¤çš„é¢˜ç›®ID")
    
    # æ£€æŸ¥æ¯ä¸ªé¢˜ç›®çš„å®Œæ•´æ€§
    for idx, question in enumerate(questions):
        if not question.get("id"):
            errors.append(f"é¢˜ç›® {idx} ç¼ºå°‘ID")
        if not question.get("question"):
            errors.append(f"é¢˜ç›® {idx} ç¼ºå°‘é¢˜ç›®æ–‡æœ¬")
        if not question.get("options") or len(question["options"]) < 2:
            errors.append(f"é¢˜ç›® {idx} é€‰é¡¹æ•°é‡ä¸è¶³")
    
    return len(errors) == 0, errors

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("ğŸ”„ KPPé¢˜ç›®æ•°æ®æ±‡æ€»å·¥å…·")
    print("=" * 60)
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    DATA_DIR.mkdir(parents=True, exist_ok=True)
    TRANSLATIONS_DIR.mkdir(parents=True, exist_ok=True)
    
    # æ±‡æ€»é¢˜ç›®
    print("\nğŸ“¦ å¼€å§‹æ±‡æ€»é¢˜ç›®...")
    all_questions, question_id_map = merge_questions()
    
    if not all_questions:
        print("âš ï¸  æ²¡æœ‰é¢˜ç›®å¯æ±‡æ€»")
        return
    
    print(f"âœ“ æ±‡æ€»å®Œæˆï¼Œå…± {len(all_questions)} é“é¢˜ç›®")
    
    # éªŒè¯æ•°æ®
    print("\nğŸ” éªŒè¯æ•°æ®...")
    is_valid, errors = validate_merged_data(all_questions)
    
    if not is_valid:
        print("âš ï¸  æ•°æ®éªŒè¯å¤±è´¥:")
        for error in errors:
            print(f"  - {error}")
        return
    
    print("âœ“ æ•°æ®éªŒè¯é€šè¿‡")
    
    # ä¿å­˜æ±‡æ€»åçš„é¢˜ç›®æ•°æ®
    print(f"\nğŸ’¾ ä¿å­˜æ±‡æ€»æ•°æ®åˆ°: {OUTPUT_FILE}")
    output_data = {
        "total": len(all_questions),
        "questions": all_questions
    }
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(output_data, f, indent=2, ensure_ascii=False)
    print("âœ“ é¢˜ç›®æ•°æ®å·²ä¿å­˜")
    
    # æ›´æ–°ç¿»è¯‘æ–‡ä»¶
    print(f"\nğŸŒ æ›´æ–°ç¿»è¯‘æ•°æ®...")
    updated_translations = update_translations(question_id_map)
    if updated_translations:
        with open(OUTPUT_TRANSLATIONS_FILE, "w", encoding="utf-8") as f:
            json.dump(updated_translations, f, indent=2, ensure_ascii=False)
        print(f"âœ“ ç¿»è¯‘æ•°æ®å·²æ›´æ–°: {OUTPUT_TRANSLATIONS_FILE}")
        print(f"  åŒ…å« {len(updated_translations.get('questions', {}))} ä¸ªé¢˜ç›®çš„ç¿»è¯‘")
    else:
        print("âš ï¸  æ²¡æœ‰ç¿»è¯‘æ•°æ®éœ€è¦æ›´æ–°")
    
    # è¾“å‡ºç»Ÿè®¡ä¿¡æ¯
    print("\n" + "=" * 60)
    print("ğŸ“Š æ±‡æ€»ç»Ÿè®¡:")
    print(f"  æ€»é¢˜ç›®æ•°: {len(all_questions)}")
    print(f"  è¾“å‡ºæ–‡ä»¶: {OUTPUT_FILE}")
    print(f"  ç¿»è¯‘æ–‡ä»¶: {OUTPUT_TRANSLATIONS_FILE}")
    print("=" * 60)

if __name__ == "__main__":
    main()
